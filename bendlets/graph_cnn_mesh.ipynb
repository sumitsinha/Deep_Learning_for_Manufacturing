{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bendlets as an intermediate data representation\n",
    "#Trasnferable and Interpretable\n",
    "#Bendlets as Graph\n",
    "#Inductive bias about the assembly process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 2.1.0\n",
      "Sonnet version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "#Importing Required Modules\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "import graph_nets as gn\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))\n",
    "print(\"Sonnet version {}\".format(snt.__version__))\n",
    "\n",
    "from graph_nets import blocks\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "import stellargraph as sg\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Nvidia Quadro GV100\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # Nvidia Quadro M2000\n",
    "\n",
    "#Adding Path to various Modules\n",
    "sys.path.append(\"../core\")\n",
    "sys.path.append(\"../visualization\")\n",
    "sys.path.append(\"../utilities\")\n",
    "sys.path.append(\"../datasets\")\n",
    "sys.path.append(\"../trained_models\")\n",
    "sys.path.append(\"../config\")\n",
    "\n",
    "#Importing Config files\n",
    "import assembly_config as config\n",
    "import model_config as cftrain\n",
    "import hybrid_utils as hy_util\n",
    "\n",
    "#Importing required modules from the package\n",
    "from measurement_system import HexagonWlsScanner\n",
    "from assembly_system import VRMSimulationModel\n",
    "from wls400a_system import GetInferenceData\n",
    "from data_import import GetTrainData\n",
    "from encode_decode_model import Encode_Decode_Model\n",
    "from training_viz import TrainViz\n",
    "from metrics_eval import MetricsEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47037</th>\n",
       "      <td>11873</td>\n",
       "      <td>11874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47038</th>\n",
       "      <td>11874</td>\n",
       "      <td>10593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47039</th>\n",
       "      <td>11874</td>\n",
       "      <td>10594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47040</th>\n",
       "      <td>11874</td>\n",
       "      <td>11008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47041</th>\n",
       "      <td>11874</td>\n",
       "      <td>11873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47042 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  target\n",
       "0           0       1\n",
       "1           0      51\n",
       "2           0     201\n",
       "3           0     219\n",
       "4           1       0\n",
       "...       ...     ...\n",
       "47037   11873   11874\n",
       "47038   11874   10593\n",
       "47039   11874   10594\n",
       "47040   11874   11008\n",
       "47041   11874   11873\n",
       "\n",
       "[47042 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting Mesh Connectivity\n",
    "\n",
    "adj_mat=loadmat('./resources/adj_mat.mat')\n",
    "adj_csr=adj_mat['adj_mat'].tocsr()\n",
    "adj_csr_n = adj_csr.nonzero()\n",
    "sparserows = adj_csr_n [0]\n",
    "sparsecols = adj_csr_n [1]\n",
    "\n",
    "edges = pd.DataFrame(\n",
    "    {\"source\":sparserows , \"target\": sparsecols}\n",
    ")\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing from Assembly Config File....\n",
      "Parsing Complete....\n"
     ]
    }
   ],
   "source": [
    "print('Parsing from Assembly Config File....')\n",
    "\n",
    "data_type=config.assembly_system['data_type']\n",
    "application=config.assembly_system['application']\n",
    "part_type=config.assembly_system['part_type']\n",
    "part_name=config.assembly_system['part_name']\n",
    "data_format=config.assembly_system['data_format']\n",
    "assembly_type=config.assembly_system['assembly_type']\n",
    "assembly_kccs=config.assembly_system['assembly_kccs']\n",
    "assembly_kpis=config.assembly_system['assembly_kpis']\n",
    "voxel_dim=config.assembly_system['voxel_dim']\n",
    "point_dim=config.assembly_system['point_dim']\n",
    "voxel_channels=config.assembly_system['voxel_channels']\n",
    "noise_type=config.assembly_system['noise_type']\n",
    "mapping_index=config.assembly_system['mapping_index']\n",
    "\n",
    "system_noise=config.assembly_system['system_noise']\n",
    "aritifical_noise=config.assembly_system['aritifical_noise']\n",
    "data_folder=config.assembly_system['data_folder']\n",
    "kcc_folder=config.assembly_system['kcc_folder']\n",
    "kcc_files=config.assembly_system['kcc_files']\n",
    "test_kcc_files=config.assembly_system['test_kcc_files']\n",
    "\n",
    "print('Parsing Complete....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing from Training Config File\n"
     ]
    }
   ],
   "source": [
    "#added for hybrid model\n",
    "categorical_kccs=config.assembly_system['categorical_kccs']\n",
    "\n",
    "print('Parsing from Training Config File')\n",
    "\n",
    "model_type=cftrain.model_parameters['model_type']\n",
    "output_type=cftrain.model_parameters['output_type']\n",
    "batch_size=cftrain.model_parameters['batch_size']\n",
    "epocs=cftrain.model_parameters['epocs']\n",
    "split_ratio=cftrain.model_parameters['split_ratio']\n",
    "optimizer=cftrain.model_parameters['optimizer']\n",
    "loss_func=cftrain.model_parameters['loss_func']\n",
    "regularizer_coeff=cftrain.model_parameters['regularizer_coeff']\n",
    "activate_tensorboard=cftrain.model_parameters['activate_tensorboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file Structure....\n"
     ]
    }
   ],
   "source": [
    "print('Creating file Structure....')\n",
    "\n",
    "bn_model_name='bendlets'\n",
    "\n",
    "folder_name=part_type\n",
    "train_path='../trained_models/'+part_type\n",
    "pathlib.Path(train_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "train_path=train_path+'/'+bn_model_name\n",
    "pathlib.Path(train_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "model_path=train_path+'/models'\n",
    "pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logs_path=train_path+'/logs'\n",
    "pathlib.Path(logs_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plots_path=train_path+'/plots'\n",
    "pathlib.Path(plots_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Assembly System and Measurement System....\n",
      "Valid Output Stages and heads\n",
      "KCC sub-listing:  0\n",
      "Process Parameter Dimension:  12\n"
     ]
    }
   ],
   "source": [
    "#Objects of Measurement System, Assembly System, Get Inference Data\n",
    "print('Initializing the Assembly System and Measurement System....')\n",
    "measurement_system=HexagonWlsScanner(data_type,application,system_noise,part_type,data_format)\n",
    "vrm_system=VRMSimulationModel(assembly_type,assembly_kccs,assembly_kpis,part_name,part_type,voxel_dim,voxel_channels,point_dim,aritifical_noise)\n",
    "get_data=GetTrainData()\n",
    "\n",
    "kcc_sublist=cftrain.encode_decode_params['kcc_sublist']\n",
    "output_heads=cftrain.encode_decode_params['output_heads']\n",
    "encode_decode_multi_output_construct=config.encode_decode_multi_output_construct\n",
    "\n",
    "if(output_heads==len(encode_decode_multi_output_construct)):\n",
    "\tprint(\"Valid Output Stages and heads\")\n",
    "else:\n",
    "\tprint(\"Inconsistent model setting\")\n",
    "\n",
    "print(\"KCC sub-listing: \",kcc_sublist)\n",
    "\n",
    "#Check for KCC sub-listing\n",
    "if(kcc_sublist!=0):\n",
    "\toutput_dimension=len(kcc_sublist)\n",
    "else:\n",
    "\toutput_dimension=assembly_kccs\n",
    "\n",
    "print(\"Process Parameter Dimension: \",output_dimension)\n",
    "\n",
    "input_size=(voxel_dim,voxel_dim,voxel_dim,voxel_channels)\n",
    "\n",
    "model_depth=cftrain.encode_decode_params['model_depth']\n",
    "inital_filter_dim=cftrain.encode_decode_params['inital_filter_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all Process Parameter\n"
     ]
    }
   ],
   "source": [
    "#importing file names for model input\n",
    "input_file_names_x=config.encode_decode_construct['input_data_files_x']\n",
    "input_file_names_y=config.encode_decode_construct['input_data_files_y']\n",
    "input_file_names_z=config.encode_decode_construct['input_data_files_z']\n",
    "\n",
    "input_dataset=[]\n",
    "input_dataset.append(get_data.data_import(input_file_names_x,data_folder))\n",
    "input_dataset.append(get_data.data_import(input_file_names_y,data_folder))\n",
    "input_dataset.append(get_data.data_import(input_file_names_z,data_folder))\n",
    "\n",
    "kcc_dataset=get_data.data_import(kcc_files,kcc_folder)\n",
    "\n",
    "if(kcc_sublist!=0):\n",
    "\tprint(\"Sub-setting Process Parameters: \",kcc_sublist)\n",
    "\tkcc_dataset=kcc_dataset.iloc[:,kcc_sublist]\n",
    "\ttest_kcc_dataset=test_kcc_dataset[:,kcc_sublist]\n",
    "else:\n",
    "\tprint(\"Using all Process Parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 11875, 3)\n"
     ]
    }
   ],
   "source": [
    "#Creating Feature Matrix\n",
    "pp_dim=0\n",
    "feature_dim=voxel_channels+pp_dim\n",
    "feature_matrix=np.zeros((len(input_dataset[0]),point_dim,feature_dim))\n",
    "\n",
    "#print(feature_matrix.shape)\n",
    "for i in range(voxel_channels):\n",
    "    feature_matrix[:,:,i]=input_dataset[i].values[:,0:point_dim]\n",
    "\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating feature matrix for one graph\n",
    "def create_node_df(feature_matrix):\n",
    "    node_data = pd.DataFrame(\n",
    "    {\"x_dev\": feature_matrix[:,0], \"y_dev\": feature_matrix[:,1],\"z_dev\": feature_matrix[:,2]})\n",
    "    \n",
    "    return node_data\n",
    "\n",
    "#node_df=create_node_df(feature_matrix[0,:,:])\n",
    "#node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 273.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#create Stellar Graph Instance\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "graphs_x=[]\n",
    "\n",
    "for i in tqdm(range(len(feature_matrix))):\n",
    "    #edges are constant\n",
    "    #Node features are to be extracted\n",
    "    node_df=create_node_df(feature_matrix[i,:,:])\n",
    "    graph_mesh = StellarGraph(node_df,edges)\n",
    "    \n",
    "    graphs_x.append(graph_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels=kcc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code for scaling all outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i in range(len(sparsecols)):\n",
    "    G.add_edge(sparserows[i], sparsecols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Graph for Testing\n",
    "from matplotlib import pylab\n",
    "import networkx as nx\n",
    "\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos)\n",
    "    nx.draw_networkx_edges(graph,pos)\n",
    "    nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "    cut = 1.00\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    pylab.close()\n",
    "    del fig\n",
    "\n",
    "#Assuming that the graph g has nodes and edges entered\n",
    "#save_graph(G,\"cross_member.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "from sklearn import model_selection\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import GCNSupervisedGraphClassification\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = PaddedGraphGenerator(graphs=graphs_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generators(train_index, test_index, graph_labels, batch_size):\n",
    "    train_gen = generator.flow(\n",
    "        train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size\n",
    "    )\n",
    "    test_gen = generator.flow(\n",
    "        test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_gen, test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Deep Learning Model\n",
    "def create_graph_classification_model(generator):\n",
    "    gc_model = GCNSupervisedGraphClassification(\n",
    "        layer_sizes=[128, 64],\n",
    "        activations=[\"relu\", \"relu\"],\n",
    "        generator=generator,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    x_inp, x_out = gc_model.in_out_tensors()\n",
    "    predictions = Dense(units=128, activation=\"relu\")(x_out)\n",
    "    predictions = Dense(units=64, activation=\"relu\")(predictions)\n",
    "    predictions = Dense(units=1, activation=\"linear\")(predictions)\n",
    "\n",
    "    # Let's create the Keras model and prepare it for training\n",
    "    model = Model(inputs=x_inp, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(model, train_gen, test_gen, es, epochs):\n",
    "    history = model.fit(\n",
    "        train_gen, epochs=epochs, validation_data=test_gen, verbose=1, callbacks=[es],\n",
    "    )\n",
    "    # calculate performance on the test data and return along with history\n",
    "    test_metrics = model.evaluate(test_gen, verbose=1)\n",
    "    test_acc = test_metrics[model.metrics_names.index(\"mae\")]\n",
    "\n",
    "    return history, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2  # maximum number of training epochs\n",
    "batch_size=10\n",
    "folds = 5  # the number of folds for k-fold cross validation\n",
    "n_repeats = 1  # the number of repeats for repeated k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on fold 1 out of 5...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 14 steps, validate for 4 steps\n",
      "Epoch 1/2\n",
      " 1/14 [=>............................] - ETA: 10:14WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Dst tensor is not initialized.\n\t [[{{node IteratorGetNext/_4}}]] [Op:__inference_distributed_function_1359]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-cd61f5f23039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph_classification_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtest_accs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-b132e1d4b1e0>\u001b[0m in \u001b[0;36mtrain_fold\u001b[1;34m(model, train_gen, test_gen, es, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_fold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     history = model.fit(\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     )\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# calculate performance on the test data and return along with history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Dst tensor is not initialized.\n\t [[{{node IteratorGetNext/_4}}]] [Op:__inference_distributed_function_1359]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "test_accs = []\n",
    "\n",
    "stratified_folds = model_selection.RepeatedKFold(n_splits=folds, n_repeats=n_repeats).split(graph_labels, graph_labels)\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only='loss',save_weights_only=True,restore_best_weights=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "    print(f\"Training and evaluating on fold {i+1} out of {folds * n_repeats}...\")\n",
    "    train_gen, test_gen = get_generators(\n",
    "        train_index, test_index, graph_labels, batch_size=10\n",
    "    )\n",
    "\n",
    "    model = create_graph_classification_model(generator)\n",
    "\n",
    "    history, acc = train_fold(model, train_gen, test_gen, checkpointer, epochs)\n",
    "\n",
    "    test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
